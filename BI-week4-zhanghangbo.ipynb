{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action 1\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import BaselineOnly,KNNBasic\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "reader = Reader(line_format=\"user item rating timestamp\",sep=\",\",skip_lines=1)\n",
    "data = Dataset.load_from_file(\"./ratings.csv\",reader=reader)\n",
    "train_set = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 0.8646\n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.07   {'was_impossible': False}\n",
      "Estimating biases using als...\n",
      "RMSE: 0.8686\n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.05   {'was_impossible': False}\n",
      "Estimating biases using als...\n",
      "RMSE: 0.8649\n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.11   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "#SGD优化\n",
    "bsl_options = {\"option\":\"sgd\",\"n_epochs\":5}\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "kf = KFold(n_splits=3)\n",
    "for trainset,testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    accuracy.rmse(predictions,verbose=True)\n",
    "    uid = str(196)\n",
    "    lid = str(302)\n",
    "    pred = algo.predict(uid,lid,r_ui=4,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = '''\n",
    "战斗 => 施法  ， 结果 。\n",
    "施法 => 主语 动作 技能 \n",
    "结果 => 主语 获得 效果\n",
    "主语 => 张飞 | 关羽 | 赵云 | 典韦 | 许褚 | 刘备 | 黄忠 | 曹操 | 鲁班七号 | 貂蝉\n",
    "动作 => 施放 | 使用 | 召唤 \n",
    "技能 => 一骑当千 | 单刀赴会 | 青龙偃月 | 刀锋铁骑 | 黑暗潜能 | 画地为牢 | 守护机关 | 狂兽血性 | 龙鸣 | 惊雷之龙 | 破云之龙 | 天翔之龙\n",
    "获得 => 损失 | 获得 \n",
    "效果 => 数值 状态\n",
    "数值 => 1 | 1000 |5000 | 100 \n",
    "状态 => 法力 | 生命\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action 3\n",
    "import random\n",
    "def getGrammarDict(gram,linesplit=\"\\n\",gramsplit=\"=>\"):\n",
    "    #定义字典\n",
    "    result = {}\n",
    "    for line in gram.split(linesplit):\n",
    "        #去掉首位空格，如果危机哦那个则推出\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        expr,statement = line.split(gramsplit)\n",
    "        result[expr.strip()]=[i.split() for i in statement.split(\"|\")]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关羽使用单刀赴会，黄忠损失100法力。\n",
      "许褚施放惊雷之龙，许褚损失1法力。\n"
     ]
    }
   ],
   "source": [
    "#生成句子\n",
    "def generate(gramdict,target,isEeng=False):\n",
    "    if target not in gramdict:\n",
    "        return target\n",
    "    find = random.choice(gramdict[target])\n",
    "    blank = \"\"\n",
    "    if isEeng:\n",
    "        blank =''\n",
    "    return blank.join(generate(gramdict,t,isEeng) for t in find)\n",
    "gramdict = getGrammarDict(grammar)\n",
    "print(generate(gramdict,\"战斗\"))\n",
    "print(generate(gramdict,\"战斗\",True))\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#thinkng 1 ALS都有哪些应用场景?\n",
    "用于电影推荐，电影推荐，商品推荐等。\n",
    "原理就是给各个指标参数，判定等加权重，然后将这些训练集输入als，内部使用矩阵相乘，根据这些权重，给用户对未知，未点击的商品一个分数，\n",
    "就是喜好程度，然后根据按照喜好程度推荐给用户。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#thingk 2 ALS进行矩阵分解的时候，为什么可以并行化处理?\n",
    "因为als使用交替最小二乘法进行求解。先固定一个参数x，然后再求矩阵y。然后再固定一个矩阵Y,求解矩阵X。\n",
    " 在求解的过程中，比如固定Y，求解X的时候，X的每一行都是独立求解，这样得话，对于每一步来说，X,Y的行和列都是可以独立计算并求解的，这样就实现了并行化处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#thinking3 梯度下降法中的批量梯度下降（BGD），随机梯度下降（SGD），和小批量梯度下降有什么区别（MBGD)\n",
    "批量梯度下降：每次迭代更新所有的样本，利用矩阵进行操作，利用矩阵进行计算，实现了并行，训练的速度很慢。\n",
    "随机梯度下降：每次迭代更新一个样本，训练的速度最快，最快收敛，准确度下降。\n",
    "小批量梯度下降：是对梯度下降和批量梯度下降一个折中的方法，每一个批量可以大大减小收敛所需要的迭代次数，可以收敛到结果更加接近梯度下降的结果。速度适中\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#thinking 4 你阅读过和推荐系统/计算广告/预测相关的论文么？\n",
    "毕业工作几年，几乎没有读过论文，近一周读了两篇。1.基于矩阵分解的协同过滤算法的并行化研究与是实现 2.基于协同过滤的个性化新闻推荐的研究和实现"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
